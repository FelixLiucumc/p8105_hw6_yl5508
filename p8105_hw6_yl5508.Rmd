---
title: "p8105_hw6_yl5508"
author: "Yifei LIU"
date: 2023/11/27
output: github_document
---

It's a proj for HW6 about LINEAR MODELS.\

```{r setup, message = FALSE}
library(tidyverse)
library(purrr)
library(modelr)
set.seed(1)
```

## Problem 1

```{r datacleaning_p1, message = FALSE}
vic_clean =
  read_csv("./data/homicide-data.csv") |>
  janitor::clean_names() |>
  mutate(city_state = str_c(city, ", ", state)) |>
  mutate(result = case_when(
    disposition == "Closed by arrest" ~ "solved",
    disposition == "Closed without arrest" | disposition == "Open/No arrest" ~ "unsolved"
  )) |>
  filter(city_state != "Dallas, TX") |>
  filter(city_state != "Phoenix, AZ") |>
  filter(city_state != "Kansas City, MO") |>
  filter(city_state != "Tulsa, AL") |>
  filter(victim_race == "White" | victim_race == "Black")
```

## Problem 2

```{r message = FALSE, warning = FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```


```{r message = FALSE, warning = FALSE}
temp_boot =
  weather_df |>
  modelr::bootstrap(n = 5000) |>
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin + prcp, data = df)),
    results = map(models, broom::tidy),
    r_squared = map(models, broom::glance)
  ) |>
  unnest(results) |>
  select(id = .id, term, estimate, r_squared) |>
  pivot_wider(names_from = term, values_from = estimate) |>
  unnest(r_squared) |>
  select(id, r_squared = r.squared, beta_1 = tmin, beta_2 = prcp) |>
  mutate(log_beta = log(beta_1 * beta_2))

#show generated results
temp_boot |>
  head(10) |>
  knitr::kable(digits = 3)
```


```{r}
#report # of NA in log calculation
tibble(
  count_na = sum(is.na(temp_boot$log_beta)),
  count_total = nrow(temp_boot),
  percent_na = sum(is.na(temp_boot$log_beta)) / nrow(temp_boot)
  ) |>
  knitr::kable()
```

Comment: There are a total of `r sum(is.na(temp_boot$log_beta))` NA in all log calculations, accounting for `r round(sum(is.na(temp_boot$log_beta)) / nrow(temp_boot)*100, 1)`% of the overall results.

```{r}
#trim NA off
temp_boot_tidy =
  temp_boot |>
  drop_na()
```


```{r}
temp_boot |>
  ggplot(aes(x = r_squared)) +
  geom_density() +
  labs(x = "R^2 Estimates", y = "Density", title = "Distribution of R Squared Estimates") +
  theme_minimal()
```

Comment: The distribution of r squared is approximately normal, left-skewed if being rigorous. The mean value of r squared would be about 0.92, and most of the sample points lie in a range larger than 0.9 indicating that there is a strong linear relation shown by the model.   

```{r}
temp_boot_tidy |>
  ggplot(aes(x = log_beta)) +
  geom_density() +
  labs(x = "log(β1*β2) Estimates", y = "Density", title = "Distribution of log(β1*β2) Estimates") +
  theme_minimal()
```

Comment: The sample points are relatively concentrated, predominantly distributed within the range of -6 to -5. There are a considerable number of outliers on the left side, indicating a left-skewed distribution.  

```{r}
temp_boot |> 
  summarize(
    ci_lower_r2 = quantile(r_squared, 0.025), 
    ci_upper_r2 = quantile(r_squared, 0.975)) |>
  knitr::kable(digits = 3)

temp_boot_tidy |> 
  summarize(
    ci_lower_log = quantile(log_beta, 0.025, na.rm = TRUE), 
    ci_upper_log = quantile(log_beta, 0.975, na.rm = TRUE)) |>
  knitr::kable(digits = 3)
```

Comment: The 95% confidence interval for r^2 is (`r round(quantile(temp_boot$r_squared, 0.025), 1)`, `r round(quantile(temp_boot$r_squared, 0.975), 1)`). The 95% confidence interval for r^2 is (`r round(quantile(temp_boot_tidy$log_beta, 0.025), 1)`, `r round(quantile(temp_boot_tidy$log_beta, 0.975), 1)`).  

## Problem 3

```{r message = FALSE, warning = FALSE}
bw_clean =
  read_csv("./data/birthweight.csv") |>
  janitor::clean_names() |>
  mutate(
    babysex = case_when(
      babysex == 1 ~ "male",
      babysex == 2~ "female"),
    frace = case_when(
      frace == 1 ~ "White",
      frace == 2 ~ "Black",
      frace == 3 ~ "Asian",
      frace == 4 ~ "Puerto Rican",
      frace == 8 ~ "Other",
      frace == 9 ~ "Unknown"),
    malform = case_when(
      malform == 0 ~ 'absent',
      malform == 1 ~ 'present'),
    mrace = case_when(
      mrace == 1 ~ "White",
      mrace == 2 ~ "Black",
      mrace == 3 ~ "Asian",
      mrace == 4 ~ "Puerto Rican",
      mrace == 8 ~ "Other")
  ) |>
  select(-pnumlbw, -pnumsga)

sum(is.na(bw_clean))
```

Comment: No NA in this dataset. No need for trimming.  

```{r}
model_bw = lm(bwt ~ ., data = bw_clean) |>
  step(direction = "both", trace = FALSE)
summary(model_bw) |>
  broom::tidy() |>
  knitr::kable(digits = 3)
```

Comment: I attempted a data-driven modeling approach by initially introducing all variables into a linear regression model. Then, employing a stepwise "both" selection method, the function evaluated the importance of variables within the model (p_value). Ultimately, the model results included variables babysex, bhead, blength, delwt, fincome, gaweeks, mheight, mrace, parity, ppwt, smoken.  

```{r message = FALSE, warning = FALSE}
res = bw_clean |>
  modelr::add_predictions(model_bw) |>
  modelr::add_residuals(model_bw)

res |>
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = .5) +
  stat_smooth(method = "lm") +
  labs(x = "Predictions", y = "Residuals", title = "Residuals against Predictions") +
  theme_minimal()
```


```{r}
model_com_1 = lm(bwt ~ blength + gaweeks, data = bw_clean)
summary(model_com_1)

model_com_2 = lm(bwt ~ bhead * blength * babysex, data = bw_clean)
summary(model_com_2)
```


```{r}
cv_bw =
  modelr::crossv_mc(bw_clean, 100)

cv_bw_rmse =
  cv_bw |>
  mutate(
    model_cv_full = map(cv_bw$train, 
                         \(df) lm(bwt ~ ., data = (df)) |>
                           step(direction = "both", trace = FALSE)),
    model_sample1 = map(cv_bw$train, 
                         \(df) lm(bwt ~ blength + gaweeks, data = (df))),
    model_sample2 = map(cv_bw$train, 
                         \(df) lm(bwt ~ bhead * blength * babysex, data = (df)))
  ) |>
  mutate(
    errs_cv_full = purrr::map2_dbl(model_cv_full, cv_bw$test, rmse),
    errs_sample1 = purrr::map2_dbl(model_sample1, cv_bw$test, rmse),
    errs_sample2 = purrr::map2_dbl(model_sample2, cv_bw$test, rmse)
  )

cv_bw_rmse |>
  select(model_gen = errs_cv_full,
         model_sample1 = errs_sample1,
         model_sample_2 = errs_sample2) |>
  pivot_longer(everything(), 
               names_to = "models",
               values_to = "rmse") |>
  mutate(model = factor(models, levels = c("model_gen", "model_sample1", "model_sample_2"))) |>
  ggplot(aes(x = model, y = rmse)) +
  geom_violin() +
  labs(x = "Models", y = "rmse", title = "Cross-Validated Prediction Error") +
  theme_minimal()
```

Comment: The model generated driven by data shows the best RMSE result. The models constructed based on empirically selected variables show poorer fit compared to the data-driven one. Specifically, the model considering only `blength and `gaweeks` exhibited the worst performance, having the highest RMSE. 








